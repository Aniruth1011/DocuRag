#from langchain_pymupdf4llm import PyMuPDF4LLMLoader
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders.parsers import RapidOCRBlobParser
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_astradb import  AstraDBVectorStore  
from langchain_milvus import Milvus
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings 
from langchain_groq import ChatGroq 
from docx import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from dotenv import load_dotenv 
from langchain import hub
import os 
import time 
from pymilvus import Collection, MilvusException,  db, utility
from langchain.prompts import PromptTemplate
import pdfplumber 
from langchain_core.documents import Document
from tqdm import tqdm


#conn = connections.connect()

URI = os.getenv("MILV_END_POINT")

def create_or_drop_database(name):
    db_name = name 
    try:
        existing_databases = db.list_database()
        if db_name in existing_databases:
            print(f"Database '{db_name}' already exists.")
            db.using_database(db_name)

            collections = utility.list_collections()
            for collection_name in collections:
                collection = Collection(name=collection_name)
                collection.drop()
                print(f"Collection '{collection_name}' has been dropped.")

            db.drop_database(db_name)
            print(f"Database '{db_name}' has been deleted.")
        else:
            print(f"Database '{db_name}' does not exist.")
            database = db.create_database(db_name)
            print(f"Database '{db_name}' created successfully.")
    except MilvusException as e:
        print(f"An error occurred: {e}")

def set_env_vars():
    load_dotenv()
    os.environ["ASTRA_DB_APPLICATION"] = os.getenv("ASTRA_DB_APPLICATION")
    os.environ["ASTRA_DB_API_ENDPOINT"] = os.getenv("ASTRA_DB_API_ENDPOINT") 
    os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY") 
    os.environ["MILV_API_KEY"] = os.getenv("MILV_API_KEY") 
    os.environ["MILV_END_POINT"] = os.getenv("MILV_END_POINT") 
    os.environ["MILV_TOKEN"] = os.getenv("MILV_TOKEN") 

def render_to_docx(question, answer):

    from docx import Document

    doc = Document()
    doc.add_heading("Q&A Generated by RAG System", 0)
    doc.add_paragraph(f"Question: {question}")
    doc.add_paragraph(f"Answer: {answer}")
    doc.save("output.docx")


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def handle_documents_and_qa(filepaths, question=None, indexing_method="flat"):

    set_env_vars()

    documents = []
    for filepath in tqdm(filepaths):
        loader = PyMuPDFLoader(filepath , mode = 'page' ,images_inner_format="markdown-img",images_parser=RapidOCRBlobParser(), extract_tables="markdown") 
        pages = []
        for page in loader.lazy_load():
            pages.append(page)      
        
        documents.extend(pages)

    # table_docs = []

    # for filepath in filepaths:
    #     with pdfplumber.open(filepath) as pdf:
    #         for i, page in enumerate(pdf.pages):
    #             tables = page.extract_tables()
    #             for table in tables:
    #                 table_text = "\n".join(["\t".join(row) for row in table if row])
    #                 table_docs.append(Document(page_content=table_text, metadata={"source": "pdf", "page": i}))


    # documents = text_documents + table_docs

    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = text_splitter.split_documents(documents)

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001") 



    # vector_store = AstraDBVectorStore(
    #     embedding=embeddings,
    #     collection_name="rag_collection",
    #     token=os.environ["ASTRA_DB_APPLICATION"],
    #     api_endpoint=os.environ["ASTRA_DB_API_ENDPOINT"],
    #     namespace=os.environ.get("ASTRA_DB_KEYSPACE", "default_keyspace")
    # )

    if indexing_method == "flat":
        vector_store = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": URI, "token":os.environ["MILV_TOKEN"], "db_name": "milvus_flat"},
            index_params={"index_type": "FLAT", "metric_type": "L2"},
            consistency_level="Strong",
            drop_old=False , 
            auto_id = True
        )
    elif indexing_method == "hnsw":
        vector_store = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": URI, "token": os.environ["MILV_TOKEN"], "db_name": "milvus_hnsw"}, 
            index_params={"index_type": "HNSW", "metric_type": "L2", "params": {"M": 16, "efConstruction": 200}}, 
            consistency_level="Strong",
            drop_old=False , auto_id = True

            )
    elif indexing_method == "ivf":
        vector_store = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": URI, "token": os.environ["MILV_TOKEN"], "db_name": "milvus_ivf"},
            index_params={"index_type": "IVF_FLAT", "metric_type": "L2", "params": {"nlist": 100}},
            consistency_level="Strong",
            drop_old=False ,             auto_id = True
)
    
    vector_store.add_documents(split_docs)


def answer_question(question="", indexing_method="flat" , retreival_method="MMR"):


    #prompt = hub.pull("rlm/rag-prompt") 

    prompt = PromptTemplate(
                input_variables=["context", "question"],
                template="""
            You are a helpful AI assistant. Use the following context to answer the user's question.
            If you cannot find the answer in the context, say "I don't know".
            Answer the question based on the context provided. Provide lots of details and be thorough in your response.

            Context:
            {context}

            Question:
            {question}

            Answer:
            """,
            )


    model = ChatGroq(model = "gemma2-9b-it", temperature=0.0)

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")


    # vector_store_loaded = AstraDBVectorStore(
    #     embedding=embeddings,
    #     collection_name="rag_collection",
    #     token=os.environ["ASTRA_DB_APPLICATION"],
    #     api_endpoint=os.environ["ASTRA_DB_API_ENDPOINT"],
    #     namespace=os.environ.get("ASTRA_DB_KEYSPACE", "default_keyspace")
    # )

    if indexing_method == "flat":
        vector_store_loaded = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": URI, "token":os.environ["MILV_TOKEN"], "db_name": "milvus_flat"},
            index_params={"index_type": "FLAT", "metric_type": "L2"},
            consistency_level="Strong",
            drop_old=False
        )
    elif indexing_method == "hnsw":
        vector_store_loaded = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": URI, "token": os.environ["MILV_TOKEN"], "db_name": "milvus_hnsw"}, 
            index_params={"index_type": "HNSW", "metric_type": "L2", "params": {"M": 16, "efConstruction": 200}}, 
            consistency_level="Strong",
            drop_old=False
            )
    elif indexing_method == "ivf":
        vector_store_loaded = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": URI, "token": os.environ["MILV_TOKEN"], "db_name": "milvus_ivf"},
            index_params={"index_type": "IVF_FLAT", "metric_type": "L2", "params": {"nlist": 100}},
            consistency_level="Strong",
            drop_old=False)


    if retreival_method == "MMR":
        retriever = vector_store_loaded.as_retriever(
            search_type="mmr",
            search_kwargs={"score_threshold": 0.9}
        )
    elif retreival_method == "BM25":
        retriever = vector_store_loaded.as_retriever(
            search_type="bm25",
            search_kwargs={"score_threshold": 0.9}
        )
    else:
        retriever = vector_store_loaded.as_retriever(
            search_type="similarity",
            search_kwargs={"score_threshold": 0.9}
        )

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | model
        | StrOutputParser()
        )
    
    start = time.time()
    result = rag_chain.invoke(question)
    print(result)
    end = time.time() 

    with open("Time_Log.txt" , "a") as f:
        f.write(f"Time taken to answer the question {question }: with {indexing_method} and {retreival_method} is {end - start} seconds\n")
    #print(f"Time taken to answer the question: {end - start} seconds")

    #render_to_docx(question, result)
    
    return result 